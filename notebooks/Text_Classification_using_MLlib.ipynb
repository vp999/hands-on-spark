{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "The aim is to mainly explore Spark's Machine learning library - **MLlib**. How to go about building models using MLlib? and to demonstrate the similarities and main differencies between two powerful Machine Learning libraries: scikit-learn and Spark's MLlib.\n",
    "\n",
    "\n",
    "We all know how handy scikit-learn's API is - it is so easy to use that it can be described with 4 little words:\n",
    "\n",
    "## IMPORT - INSTANTIATE - FIT - PREDICT\n",
    "\n",
    "Most of the pipelines or algorithms within scikit-learn revolves around above words. Because this simplicity, Spark's MLlib is build with inspiration from scikit-learn's structure, so the scikit-learn user will easily be able to use Spark ML API when working on Big Data workflows is needed.\n",
    "\n",
    "So we will see simplicity of moving from scikit-learn to Spark's MLlib when working on a bigger range of data to train and use Machine Learning workflows.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "\n",
    "##### SMS Spam Collection Data Set \n",
    "\n",
    "The SMS Spam Collection is a public set of SMS labeled messages as `spam` or `ham` that have been collected for mobile phone spam research. The file contains a collection of more than 5 thousand SMS phone messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load data and look around\n",
    "\n",
    "A collection of texts is also sometimes called *corpus*. Let's print the first five messages in this SMS corpus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df = pd.read_csv('../data/SMSSpamCollection.csv')\n",
    "\n",
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus will be our labeled training set. Using these ham/spam examples, we'll **train a machine learning model to learn to discriminate between ham/spam automatically**. Then, with a trained model, we'll be able to **classify arbitrary unlabeled messages as ham or spam**.\n",
    "\n",
    "![supervised ML](../img/supervised_ML.png 'supervised ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4827\n",
       "spam     747\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark way:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('../data/SMSSpamCollection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['category', 'text']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|category|                text|\n",
      "+--------+--------------------+\n",
      "|     ham|Go until jurong p...|\n",
      "|     ham|Ok lar... Joking ...|\n",
      "|    spam|Free entry in 2 a...|\n",
      "|     ham|U dun say so earl...|\n",
      "|     ham|Nah I don't think...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|     ham| 4827|\n",
      "|    spam|  747|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# by top 20 categories\n",
    "data.groupBy(\"category\") \\\n",
    "    .count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text  label_num\n",
       "0      ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1      ham                      Ok lar... Joking wif u oni...          0\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3      ham  U dun say so early hor... U c already then say...          0\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...          0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert label to a numerical variable\n",
    "sms_df['label_num'] = sms_df.category.map({'ham':0, 'spam':1})\n",
    "sms_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5574,)\n",
      "(5574,)\n"
     ]
    }
   ],
   "source": [
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "X = sms_df.text\n",
    "y = sms_df.category\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4180,)\n",
      "(1394,)\n",
      "(4180,)\n",
      "(1394,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Vectorizing our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Using `CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer(stop_words=stopwords_list)\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> equivalently: combine fit and transform into a single step by following line of code:\n",
    "\n",
    "\n",
    "`X_train_dtm = vect.fit_transform(X_train)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4180x7334 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 35435 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1394x7334 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10648 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Document Term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['The quick brown fox jumps over the lazy dog', 'I have dog called robert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_d = CountVectorizer(stop_words=stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = vect_d.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown', 'called', 'dog', 'fox', 'jumps', 'lazy', 'quick', 'robert']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_d.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 1, 1, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brown</th>\n",
       "      <th>called</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>quick</th>\n",
       "      <th>robert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brown  called  dog  fox  jumps  lazy  quick  robert\n",
       "0      1       0    1    1      1     1      1       0\n",
       "1      0       1    1    0      0     0      0       1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=vect_d.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pyspark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(stopwords_list)\n",
    "\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|category|                text|               words|            filtered|            features|label|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|     ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|(1714,[10,14,35,5...|  0.0|\n",
      "|     ham|Ok lar... Joking ...|[ok, lar, joking,...|[ok, lar, joking,...|(1714,[0,8,247,36...|  0.0|\n",
      "|    spam|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|(1714,[2,9,21,22,...|  1.0|\n",
      "|     ham|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|(1714,[0,53,78,79...|  0.0|\n",
      "|     ham|Nah I don't think...|[nah, i, don, t, ...|[nah, think, goes...|(1714,[49,131,361...|  0.0|\n",
      "|    spam|FreeMsg Hey there...|[freemsg, hey, th...|[freemsg, hey, da...|(1714,[8,13,19,24...|  1.0|\n",
      "|     ham|Even my brother i...|[even, my, brothe...|[even, brother, l...|(1714,[13,123,279...|  0.0|\n",
      "|     ham|As per your reque...|[as, per, your, r...|[per, request, me...|(1714,[144,156,29...|  0.0|\n",
      "|    spam|WINNER!! As a val...|[winner, as, a, v...|[winner, valued, ...|(1714,[1,60,75,14...|  1.0|\n",
      "|    spam|Had your mobile 1...|[had, your, mobil...|[mobile, 11, mont...|(1714,[0,1,9,29,4...|  1.0|\n",
      "|     ham|I'm gonna be home...|[i, m, gonna, be,...|[gonna, home, soo...|(1714,[20,27,30,3...|  0.0|\n",
      "|    spam|SIX chances to wi...|[six, chances, to...|[six, chances, wi...|(1714,[5,15,19,22...|  1.0|\n",
      "|    spam|URGENT! You have ...|[urgent, you, hav...|[urgent, 1, week,...|(1714,[9,22,24,51...|  1.0|\n",
      "|     ham|I've been searchi...|[i, ve, been, sea...|[searching, right...|(1714,[42,77,143,...|  0.0|\n",
      "|     ham|I HAVE A DATE ON ...|[i, have, a, date...|      [date, sunday]|(1714,[475,676],[...|  0.0|\n",
      "|    spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(1714,[22,35,74,1...|  1.0|\n",
      "|     ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|   [oh, k, watching]|(1714,[39,59,273]...|  0.0|\n",
      "|     ham|Eh u remember how...|[eh, u, remember,...|[eh, u, remember,...|(1714,[0,2,66,69,...|  0.0|\n",
      "|     ham|Fine if that¬ís t...|[fine, if, that, ...|[fine, way, u, fe...|(1714,[0,68,85,12...|  0.0|\n",
      "|    spam|England v Macedon...|[england, v, mace...|[england, v, mace...|(1714,[4,22,24,40...|  1.0|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3964\n",
      "Test Dataset Count: 1610\n"
     ]
    }
   ],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice the differences:\n",
    "\n",
    "In Sklearn flow is like this:\n",
    "\n",
    "1. LabelEncoding\n",
    "2. Train-Test split\n",
    "3. Stopwords Removal and Tokenizer within CountVectorizer - `fit` and `transform`\n",
    "\n",
    "\n",
    "\n",
    "On other hand, Spark MLlib flow is like this:\n",
    "\n",
    "1. Regextokenizer and stopwords removal\n",
    "2. CountVectorizer - `fit` and `transform`\n",
    "3. Train-Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression using Count Vector Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_sklearn = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.9 ms, sys: 3.86 ms, total: 27.8 ms\n",
      "Wall time: 30.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lr_sklearn.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = lr_sklearn.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9856527977044476"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1210,    1],\n",
       "       [  19,  164]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pyspark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Build the model\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                          text|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|The last thing i ever wante...|     ham|[0.9999999999999682,3.17461...|  0.0|       0.0|\n",
      "|No i'm not. I can't give yo...|     ham|[0.9999999923256403,7.67435...|  0.0|       0.0|\n",
      "|K, wen ur free come to my h...|     ham|[0.9999999627400645,3.72599...|  0.0|       0.0|\n",
      "|For me the love should star...|     ham|[0.9999999539665888,4.60334...|  0.0|       0.0|\n",
      "|Oh... Haha... Den we shld h...|     ham|[0.9999998825435645,1.17456...|  0.0|       0.0|\n",
      "|Heart is empty without love...|     ham|[0.9999998445649958,1.55435...|  0.0|       0.0|\n",
      "|K, fyi I'm back in my paren...|     ham|[0.9999998395913674,1.60408...|  0.0|       0.0|\n",
      "|Thanks again for your reply...|     ham|[0.9999997954104055,2.04589...|  0.0|       0.0|\n",
      "|He neva grumble but i sad l...|     ham|[0.9999997757625653,2.24237...|  0.0|       0.0|\n",
      "|\"Wow. I never realized that...|     ham|[0.9999997234497565,2.76550...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420289855072463"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Logistic Regression using TF-IDF Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "tfidf_vect = TfidfVectorizer(stop_words=stopwords_list)\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_dtm_tfidf = tfidf_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1394x7334 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10648 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm_tfidf = tfidf_vect.transform(X_test)\n",
    "X_test_dtm_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 ms, sys: 1.74 ms, total: 18.8 ms\n",
      "Wall time: 17.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lr_sklearn.fit(X_train_dtm_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class_tfidf = lr_sklearn.predict(X_test_dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9856527977044476"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1210,    1],\n",
       "       [  19,  164]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pyspark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# Add HashingTF and IDF to transformation\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "# Redo Pipeline\n",
    "pipeline_tf = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit_tf = pipeline_tf.fit(data)\n",
    "dataset = pipelineFit_tf.transform(data)\n",
    "\n",
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "# Build the model\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.001, elasticNetParam=0)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|                          text|category|                   probability|label|prediction|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "|The last thing i ever wante...|     ham|[0.9999999999446623,5.53378...|  0.0|       0.0|\n",
      "|No i'm not. I can't give yo...|     ham|[0.9999999839674525,1.60325...|  0.0|       0.0|\n",
      "|He neva grumble but i sad l...|     ham|[0.9999999185998198,8.14001...|  0.0|       0.0|\n",
      "|THING R GOOD THANX GOT EXAM...|     ham|[0.9999999143905409,8.56094...|  0.0|       0.0|\n",
      "|For me the love should star...|     ham|[0.9999998708332496,1.29166...|  0.0|       0.0|\n",
      "|Although i told u dat i'm i...|     ham|[0.9999997789977655,2.21002...|  0.0|       0.0|\n",
      "|Thanks again for your reply...|     ham|[0.9999997672482266,2.32751...|  0.0|       0.0|\n",
      "|Wen ur lovable bcums angry ...|     ham|[0.9999997132583697,2.86741...|  0.0|       0.0|\n",
      "|Heart is empty without love...|     ham|[0.9999996745694177,3.25430...|  0.0|       0.0|\n",
      "|Dude. What's up. How Teresa...|     ham|[0.9999996603435238,3.39656...|  0.0|       0.0|\n",
      "+------------------------------+--------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(testData)\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9431159420289855"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of how to create a spark dataframe\n",
    "\n",
    "```python\n",
    "from pyspark.sql import Row\n",
    "l = [('Ankit',25),('Jalfaizy',22),('saurabh',20),('Bala',26)]\n",
    "rdd = sc.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "schemaPeople = sqlContext.createDataFrame(people)\n",
    "\n",
    "print(type(schemaPeople))\n",
    "#  pyspark.sql.dataframe.DataFrame\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "ll = [('Hurry up! Answer simple questions and WINNER will get $900 prize reward! To claim call us. Valid 12 hours only.'),('Hey, How are you? Long time no see')]\n",
    "rdds = sc.parallelize(ll)\n",
    "tx = rdds.map(lambda x: Row(text=x))\n",
    "schematxt = sqlContext.createDataFrame(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|Hurry up! Answer ...|\n",
      "|Hey, How are you?...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schematxt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating features for test sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|               words|            filtered|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Hurry up! Answer ...|[hurry, up, answe...|[hurry, answer, s...|(10000,[1,721,727...|(10000,[1,721,727...|\n",
      "|Hey, How are you?...|[hey, how, are, y...|[hey, long, time,...|(10000,[7515,8157...|(10000,[7515,8157...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_new_dataset = pipelineFit_tf.transform(schematxt)\n",
    "test_new_dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on calculated features of test sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lrModel.transform(test_new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|                text|         probability|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|Hurry up! Answer ...|[0.18778340849998...|       1.0|\n",
      "|Hey, How are you?...|[0.99950414795125...|       0.0|\n",
      "+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred.select(\"text\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model using Naive Bayes algorithm\n",
    "\n",
    "### and predict on sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "nb_model = nb.fit(trainingData)\n",
    "predictions = nb_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9641304347826087"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction on sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|                text|         probability|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|Hurry up! Answer ...|[0.18778340849998...|       1.0|\n",
      "|Hey, How are you?...|[0.99950414795125...|       0.0|\n",
      "+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred.select(\"text\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how well Naive Bayes performing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Cross Validation on dataset using Logistic Regression Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9206521739130434"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "# Build the model\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "\n",
    "# Use test set here so we can measure the accuracy of our model on new data\n",
    "predictions = cvModel.transform(testData)\n",
    "\n",
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
