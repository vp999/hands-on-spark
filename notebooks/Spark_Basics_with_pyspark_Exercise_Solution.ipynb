{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf's Law\n",
    "\n",
    "Named after the American linguist George Kingsley Zipf (1902-1950), Zipf's Law describes an empirical law that describes the phenomenon in the physical and social sciences where many types of data can be approximated by Zipf distribution - a family of discrete power law probability distributions.\n",
    "\n",
    "In linguistics, Zipf's law states that given some text of natural language, the frequency of any word is inversely proportional to its rank in the frequency table.  The most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc. \n",
    "\n",
    "For example,\n",
    "In one sample of words in the English language, the most frequently occurring word, **\"the\"**, accounts for nearly 7% of all the words (69,971 out of slightly over 1 million). True to Zipf's Law, the second-place word **\"of\"** accounts for slightly over 3.5% of words (36,411 occurrences), followed by **\"and\"** (28,852). \n",
    "\n",
    "#### Formula:\n",
    "\n",
    "Zipf's law then predicts that frequency of element of rank $k$ can be approximated by,\n",
    "\n",
    "$$f(k; s, N) = \\frac{1/k^s}{\\sum_{n=1}^N (1/n^s)}$$\n",
    "\n",
    "where $N$ is the number of elements, and $s$ is the value of the exponent characterizing the distribution.  Normally, $s$ is defined to be $1$.\n",
    "\n",
    "To see illustration of Wordcount example visit: http://www.wordcount.org/main.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda rank,elements,s: 1/(np.sum(1./(np.arange(1,elements+1)**s)) * rank**s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = np.arange(1,51)\n",
    "N = 5000\n",
    "s = 1.0\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(ranks,f(ranks,N,s), 'o-')\n",
    "plt.title(\"Zipfian Distribution PMF\")\n",
    "plt.xlabel(\"k: rank\")\n",
    "plt.ylabel(\"p: probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count from Project Gutenberg Free ebook Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This exercise will attempt to replicate the results from wordcount.org by processing a large volume of sample texts.  We'll use Spark to read the text data, process the texts, and examine the total number of unique words.  \n",
    "\n",
    "We will sample 16 texts from [Project Gutenberg](https://www.gutenberg.org/ \"Title\").\n",
    "Sample text files are in following location in repo:\n",
    "\n",
    "`../data/books/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('../data/books/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load up the text files and create a text file RDD.  Spark provides `textFile` method to read a text file and return it as a RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "books_path = os.path.join('..', 'data', 'books')\n",
    "textsRDD = sc.textFile(books_path + '/*.txt')\n",
    "# for txt in os.listdir('data')[1:]:\n",
    "#     file_dir = 'data/' + txt\n",
    "#     textRDD = sc.textFile(file_dir)\n",
    "#     textsRDD = textsRDD.union(textRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textsRDD.take(20)\n",
    "textsRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up the words\n",
    "import re\n",
    "\n",
    "def parsewords(sentence):\n",
    "    # Convert all non-alphanumeric characters into empty string\n",
    "    sentence_clean = re.sub(r'([^A-Za-z0-9\\s+])', '', sentence)  \n",
    "    words = sentence_clean.split(' ')\n",
    "    # Convert to lowercase and eliminate empty string words\n",
    "    return [word.lower() for word in words if word != ''] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a a RDD that is a collection of strings.  We'll start by mapping the function `parsewords` to each of the line of strings in `textsRDD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textsRDD.map(parsewords).take(5)  # Not quite what we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the result of mapping `parsewords` to `textsRDD` did not yield the result that we want. \n",
    "\n",
    "RDD has `flatMap` method that will map a function that has been passed in as a parameter to elements of the RDD and flatten the result.  We'll use `flatMap` method and apply `parsewords` function to create a new RDD of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordRDD = textsRDD.flatMap(parsewords)\n",
    "wordRDD.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an RDD of words, let's examine how many words are in our sample texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Number of Words in our Sample\n",
    "total_count = wordRDD.count()\n",
    "print(\"Total Word Count:\", total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `map` method of the RDD to convert each occurance of word into a (key,value) pair and create a new RDD called `wordPairRDD`.  If you saw the word count example in MapReduce, you should be familiar with this pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert individual word into a pair of (word,1) tuple \n",
    "wordPairRDD = wordRDD.map(lambda word: (word,1))\n",
    "wordPairRDD.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have converted wordRDD into a a new RDD that holds each instance of word into a (key,value) pair.  Let's use reduceByKey method to sum up the counts and create a new RDD, `wordCountRDD`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountRDD = wordPairRDD.reduceByKey(lambda x,y: x+y)\n",
    "wordCountRDD.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Unique Words\n",
    "unique_count = wordCountRDD.count()\n",
    "print(\"Total Unique Words:\", unique_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `wordCountRDD` contains unique words and their counts, we sort them by `takeOrdered` method of RDD specifying the the ordering function to order by the values in (key,value) pairs.  We will only select top 50 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Top 50 words in descending order of frequency\n",
    "top50Words = wordCountRDD.takeOrdered(50, key=lambda x: -x[1]) # Use '-' to sort in descending order\n",
    "top50Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining\n",
    "\n",
    "<img src=\"../img/word_count_RDD_transformation.jpeg\" width=\"600px\">\n",
    "\n",
    "Although we have performed each transformation in separate steps, Spark enables us to perform these four steps in one line of code by chaining them together.\n",
    "\n",
    "Although Python is strict about indentation, we use a convenient trick of wrapping a chain of RDD methods inside a parenthesis.  This trick enables us to chain multiple RDD methods in several lines of code, which enhances readability of your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top50Words = (textsRDD.flatMap(parsewords)\n",
    "                      .map(lambda word: (word,1))\n",
    "                      .reduceByKey(lambda x,y: x+y)\n",
    "                      .takeOrdered(50, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list()\n",
    "counts = list()\n",
    "for pair in top50Words:\n",
    "    words.append(pair[0])\n",
    "    counts.append(pair[1])\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(range(1,51), np.array(counts) / float(total_count), label=\"Word Frequency\", align='center')\n",
    "plt.plot(ranks, f(ranks,N,s), c='r', label=\"$f(k,s,N)$\")\n",
    "plt.xticks(range(1,51), words[:50], rotation=60, fontsize=14)\n",
    "plt.title(\"Frequency of Top 50 Words\",fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlim(0.5,50.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not exact, the word frequency from our sample of texts follows the Zipf distribution closely.  We also see that in our sample texts, \"and\" is the second most frequently occuring word.  This is in contrast to the word frequency ordering of BNC in which \"of\" is ranked second and \"and\" is ranked third.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
